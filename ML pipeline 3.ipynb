{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\"><b>Text generation</b></h1>\n",
    "<h5 style=\"text-align: center;\"><I>Generating poetry from a trained model</I></h5>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1. Executive summary**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2. Introduction**\n",
    "\n",
    "Previously I experimented with clustering and topic modeling with my poetry. I used K-means clustering and vectorization initially and found the model to perform poorly, given the limited data and model's lack of complexity to compensate for it. Next I used ELMo word embeddings and tried clustering the themes in the data with both K-means model and the Latent Drichlet Allocation (LDA) model. The LDA model performed much betters, especially when used with an autoencoder to extract features.\n",
    "\n",
    "The next step in this pipeline is to generate poetry using a text generation model. I will be using a Long Short-Term Memory (LSTM) model to generate poetry. The model will be trained on the poetry corpus and will be able to generate poetry based on the themes in the corpus."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3. Importing data and packages**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4. Preprocessing**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **5. Text generation models**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Long Short-Term Memory (LSTM) model***\n",
    "\n",
    "Long Short-Term Memory (LSTM) is a type of Recurrent Neural Network (RNN) architecture specifically designed to address the vanishing gradient problem in traditional RNNs. This problem arises when training RNNs on long sequences, as gradients either vanish or explode, making it difficult for the network to learn long-range dependencies. LSTMs are capable of learning and retaining information over long sequences, making them suitable for various sequence-to-sequence tasks like language modeling, machine translation, and text generation.\n",
    "\n",
    "The LSTM architecture introduces memory cells and additional gating mechanisms to control the flow of information within the network. The key components of an LSTM cell are:\n",
    "\n",
    "- Input gate $(i_t)$\n",
    "- Forget gate $(f_t)$\n",
    "- Output gate $(o_t)$\n",
    "- Cell state $(C_t)$\n",
    "- Hidden state $(h_t)$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematically, the LSTM cell's operations can be described as follows:\n",
    "\n",
    "**Input gate $(i_t)$:**\n",
    "$$i_t = σ(W_i[h_{t-1}, x_t] + b_i)$$\n",
    "The input gate decides how much of the new input $(x_t)$ and previous hidden state $(h_{t-1})$ should be used to update the cell state. It uses a sigmoid activation function $(σ)$ and learns the weights $(W_i)$ and biases $(b_i)$ during training.\n",
    "\n",
    "**Forget gate $(f_t)$**:\n",
    "$$f_t = σ(W_f[h_{t-1}, x_t] + b_f)$$\n",
    "The forget gate decides how much of the previous cell state $(C_{t-1})$ should be retained or forgotten. It also uses a sigmoid activation function and learns the weights $(W_f)$ and biases $(b_f)$ during training.\n",
    "\n",
    "\n",
    "**Cell state $(C_t)$:**\n",
    "$$C_t = f_t * C_{t-1} + i_t * tanh(W_c[h_{t-1}, x_t] + b_c)$$\n",
    "The cell state is the internal memory of the LSTM cell. It gets updated based on the previous cell state $(C_{t-1})$, input gate $(i_t)$, and a candidate cell state, which is a combination of the previous hidden state $(h_{t-1})$ and current input $(x_t)$ passed through a hyperbolic tangent (tanh) activation function. The weights $(W_c)$ and biases $(b_c)$ are learned during training.\n",
    "\n",
    "\n",
    "**Output gate $(o_t)$:**\n",
    "$$o_t = σ(W_o[h_{t-1}, x_t] + b_o)$$\n",
    "The output gate controls how much of the cell state $(C_t)$ is exposed to the next layer or the next time step. It uses a sigmoid activation function and learns the weights $(W_o)$ and biases $(b_o)$ during training.\n",
    "\n",
    "\n",
    "**Hidden state $(h_t)$:**\n",
    "$$h_t = o_t * tanh(C_t)$$\n",
    "The hidden state is the output of the LSTM cell at each time step. It is a combination of the output gate $(o_t)$ and the cell state $(C_t)$ passed through a hyperbolic tangent $(tanh)$ activation function."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.1.1 Training the LSTM model**\n",
    "\n",
    "**5.1.2 Testing the model**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***BERT***\n",
    "\n",
    "BERT is a pre-trained language model developed by Google. It is a bidirectional transformer-based model that uses a self-attention mechanism to learn contextual representations of words. The model is trained on a large corpus of text and can be fine-tuned on a variety of downstream tasks. \n",
    "\n",
    "BERT is built upon the Transformer architecture, which was introduced by Vaswani et al. in their paper \"Attention is All You Need.\" The key components of the BERT model are:\n",
    "\n",
    "1. Multi-Head Self-Attention Mechanism\n",
    "2. Position-wise Feed-Forward Networks\n",
    "3. Layer Normalization\n",
    "4. Positional Encoding\n",
    "\n",
    "BERT is a very advanced and complicated model but a high level pseudoalgorithm of the model is as follows:\n",
    "\n",
    "1. **Load pre-trained BERT model and tokenizer**.\n",
    "\n",
    "2. **Tokenize input text**:\n",
    "<br>\n",
    "    a. Add special tokens [CLS] at the beginning and [SEP] at the end of the text.\n",
    "    <br>\n",
    "    b. Apply WordPiece tokenization to the input text.\n",
    "    <br> c. Convert tokens into token IDs using the tokenizer's vocabulary.\n",
    "\n",
    "3. **Create input features**:<br>\n",
    "    a. Token IDs: The sequence of token IDs obtained from step 2.c.<br>\n",
    "    b. Segment IDs: A binary mask to differentiate between different sentences (0 for the first sentence and 1 for the   second sentence, if applicable).<br>\n",
    "    c. Position IDs: A sequence of integers representing the position of each token in the input.<br>\n",
    "    d. Attention Mask: A binary mask indicating the positions of non-padding tokens.<br>\n",
    "\n",
    "4. **Forward pass through the BERT model**:<br>\n",
    "    a. Embed token IDs, segment IDs, and position IDs.<br>\n",
    "    b. Pass the embeddings through a stack of Transformer encoder layers.<br>\n",
    "    c. For each layer:<br>\n",
    "    * i. Apply multi-head self-attention mechanism.<br>\n",
    "    * ii. Add the attention output to the input and apply layer normalization.<br>\n",
    "    * iii. Pass the result through a position-wise feed-forward network.<br>\n",
    "    * iv. Add the feed-forward output to the input and apply layer normalization.<br>\n",
    "    \n",
    "    d. Obtain the final output representations for each token.<br>\n",
    "\n",
    "5. **Fine-tune BERT for a specific task (if necessary):**<br>\n",
    "    a. Add a task-specific output layer on top of the BERT model.<br>\n",
    "    b. Train the model using labeled data for the target task, updating the weights with backpropagation.<br>\n",
    "\n",
    "6. **Perform inference for the target task:**<br>\n",
    "    a. For classification tasks, use the representation of the [CLS] token and pass it through the task-specific output layer.<br>\n",
    "    b. For token-level tasks, use the representations of the individual tokens and pass them through the task-specific output layer.<br>\n",
    "    c. Apply the appropriate activation function and compute the final predictions.<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.2.1 Training the BERT model**\n",
    "\n",
    "**5.2.2 Testing the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **6. Discussion**\n",
    "\n",
    "Both the models show some ver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
